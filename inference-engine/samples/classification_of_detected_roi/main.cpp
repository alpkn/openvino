// Copyright (C) 2020 Intel Corporation
// SPDX-License-Identifier: Apache-2.0
//

#include "classification_of_detected_roi.h"

#include <samples/common.hpp>
#include <samples/slog.hpp>
#include <samples/args_helper.hpp>

#include <inference_engine.hpp>
#include <vpu/vpu_tools_common.hpp>
#include <format_reader_ptr.h>

#include <gflags/gflags.h>

#include <iostream>
#include <string>
#include <memory>
#include <vector>
#include <algorithm>
#include <map>
#include <mutex>
#include <condition_variable>

using namespace InferenceEngine;

bool ParseAndCheckCommandLine(int argc, char *argv[]) {
    gflags::ParseCommandLineNonHelpFlags(&argc, &argv, true);

    if (FLAGS_h) {
        showUsage();
        showAvailableDevices();
        return false;
    }

    slog::info << "Parsing input parameters" << slog::endl;

    if (FLAGS_i.empty()) {
        throw std::logic_error("Parameter -i is not set");
    }
    if (FLAGS_det_model.empty()) {
        throw std::logic_error("Parameter -det_model is not set");
    }
    if (FLAGS_cls_model.empty()) {
        throw std::logic_error("Parameter -cls_model is not set");
    }

    return true;
}

std::string getFileNameFromPath(const std::string& path,
#if defined(_WIN32)
                                const std::string sep = "\\") {
#else
                                const std::string sep = "/") {
#endif
    auto pos = path.rfind(sep);
    if (std::string::npos == pos) {
        return path;
    } else {
        return path.substr(pos + 1);
    }
}

/**
 * \brief The entry point for the Inference Engine classification_of_detected_roi sample application
 * \file classification_of_detected_roi/main.cpp
 * \example classification_of_detected_roi/main.cpp
 */
int main(int argc, char *argv[]) {
    try {
        //
        // Parse and validate input args
        //

        if (!ParseAndCheckCommandLine(argc, argv)) {
            return 0;
        }

        std::vector<std::string> images;
        parseInputFilesArguments(images);

        if (images.empty()) {
            throw std::logic_error("No suitable images were found");
        }

        std::vector<std::string> clsLabels;
        if (!FLAGS_cls_labels.empty()) {
            std::ifstream file(FLAGS_cls_labels);
            if (!file.is_open()) {
                throw std::logic_error("Can't open file with classification labels");
            }

            for (std::string line; std::getline(file, line); ) {
                clsLabels.push_back(std::move(line));
            }
        }

        //
        // Load input images
        //

        slog::info << "Load input images" << slog::endl;

        std::vector<std::shared_ptr<unsigned char>> imagesData;
        std::vector<size_t> imagesWidth;
        std::vector<size_t> imagesHeight;

        imagesData.reserve(images.size());
        imagesWidth.reserve(images.size());
        imagesHeight.reserve(images.size());

        for (const auto& imageFileName : images) {
            FormatReader::ReaderPtr reader(imageFileName.c_str());

            if (reader.get() == nullptr) {
                slog::warn << "    Image " + imageFileName + " cannot be read!" << slog::endl;
                continue;
            }

            imagesData.push_back(reader->getData());
            imagesWidth.push_back(reader->width());
            imagesHeight.push_back(reader->height());
        }

        //
        // Load inference engine
        //

        slog::info << "Load Inference Engine" << slog::endl;

        Core ie;

        slog::info << "    InferenceEngine: " << GetInferenceEngineVersion() << slog::endl;

        slog::info << "    Device info: " << slog::endl;
        slog::info << ie.GetVersions(FLAGS_device) << slog::endl;

        if (!FLAGS_custom_cpu.empty()) {
            // CPU(MKLDNN) extensions are loaded as a shared library and passed as a pointer to base extension

            const auto extension_ptr = make_so_pointer<IExtension>(FLAGS_custom_cpu);
            ie.AddExtension(extension_ptr);

            slog::info << "    CPU Extension loaded: " << FLAGS_custom_cpu << slog::endl;
        }

        if (!FLAGS_custom_cldnn.empty()) {
            // clDNN Extensions are loaded from an .xml description and OpenCL kernel files

            ie.SetConfig({ { PluginConfigParams::KEY_CONFIG_FILE, FLAGS_custom_cldnn } }, "GPU");

            slog::info << "    GPU Extension loaded: " << FLAGS_custom_cldnn << slog::endl;
        }

        //
        // Read IRs generated by ModelOptimizer (.xml and .bin files)
        //

        slog::info << "Load network IRs:" << slog::endl
            << "\t" << FLAGS_det_model << slog::endl
            << "\t" << FLAGS_cls_model << slog::endl;

        CNNNetwork detNetwork = ie.ReadNetwork(FLAGS_det_model);
        CNNNetwork clsNetwork = ie.ReadNetwork(FLAGS_cls_model);

        //
        // Prepare input info
        //

        slog::info << "Prepare input info" << slog::endl;

        std::string detInputName;
        {
            const auto detInputsInfo = detNetwork.getInputsInfo();

            // SSD network has one input or two inputs (image info)
            if (detInputsInfo.size() != 1) {
                throw std::logic_error("Sample supports only detection topologies with 1 input");
            }

            const auto& item = *detInputsInfo.begin();
            const auto& itemInfo = item.second;
            const auto& itemData = itemInfo->getInputData();
            const auto& itemDims = itemData->getTensorDesc().getDims();

            if (itemDims.size() != 4) {
                throw std::logic_error("Unsupported detection network input");
            }

            detInputName = itemData->getName();
            itemData->setPrecision(Precision::U8);

            itemInfo->getPreProcess().setResizeAlgorithm(RESIZE_AREA);
        }

        std::string clsInputName;
        {
            const auto clsInputsInfo = clsNetwork.getInputsInfo();

            if (clsInputsInfo.size() != 1) {
                throw std::logic_error("Sample supports only classification topologies with 1 input");
            }

            const auto& item = *clsInputsInfo.begin();
            const auto& itemInfo = item.second;
            const auto& itemData = itemInfo->getInputData();
            const auto& itemDims = itemData->getTensorDesc().getDims();

            if (itemDims.size() != 4) {
                throw std::logic_error("Unsupported classification network input");
            }

            clsInputName = itemData->getName();
            itemData->setPrecision(Precision::U8);

            itemInfo->getPreProcess().setResizeAlgorithm(RESIZE_AREA);
        }

        //
        // Prepare output info
        //

        slog::info << "Prepare output info" << slog::endl;

        std::string detOutputName;
        {
            const auto detOutputsInfo = detNetwork.getOutputsInfo();

            if (detOutputsInfo.size() != 1) {
                throw std::logic_error("Sample supports only detection topologies with 1 output");
            }

            const auto& item = *detOutputsInfo.begin();
            const auto& itemData = item.second;
            const auto& itemDims = itemData->getTensorDesc().getDims();

            if (itemDims.size() != 4) {
                throw std::logic_error("Unsupported detection network output");
            }

            const auto objectSize = itemDims[3];

            if (objectSize != 7) {
                throw std::logic_error("Detection output item should have 7 as a last dimension");
            }

            detOutputName = itemData->getName();
            itemData->setPrecision(Precision::FP32);
        }

        std::string clsOutputName;
        {
            const auto clsOutputsInfo = clsNetwork.getOutputsInfo();

            if (clsOutputsInfo.size() != 1) {
                throw std::logic_error("Sample supports only classification topologies with 1 output");
            }

            const auto& item = *clsOutputsInfo.begin();
            const auto& itemData = item.second;
            const auto& itemDims = itemData->getTensorDesc().getDims();

            if (itemDims.size() != 2) {
                throw std::logic_error("Unsupported classification network output");
            }
            if (!clsLabels.empty()) {
                if (clsLabels.size() != itemDims[1]) {
                    throw std::logic_error("Number of labels doesn't match classification network output");
                }
            }

            clsOutputName = itemData->getName();
            itemData->setPrecision(Precision::FP32);
        }

        //
        // Load models to the device
        //

        slog::info << "Load models to the device" << slog::endl;

        const auto netConfig = parseConfig(FLAGS_config);

        auto detExeNet = ie.LoadNetwork(detNetwork, FLAGS_device, netConfig);
        auto clsExeNet = ie.LoadNetwork(clsNetwork, FLAGS_device, netConfig);

        //
        // Create infer requests
        //

        slog::info << "Create infer requests" << slog::endl;

        auto detInferRequest = detExeNet.CreateInferRequest();

        const auto numInferRequests = clsExeNet.GetMetric(METRIC_KEY(OPTIMAL_NUMBER_OF_INFER_REQUESTS)).as<unsigned int>();

        slog::info << "    Will use " << numInferRequests << " parallel async infer request(s) for classification" << slog::endl;

        std::vector<InferRequest> clsInferRequests(numInferRequests);
        for (auto& clsInferRequest : clsInferRequests) {
            clsInferRequest = clsExeNet.CreateInferRequest();
        }

        //
        // Setup detection+classification pipeline
        //

        const auto getDetectionROIs =
            [](
                    MemoryBlob::Ptr detectionsBlob,
                    Blob::Ptr imageBlob)
                    -> std::vector<Blob::Ptr> {
                std::vector<Blob::Ptr> ROIs;
                const auto& imageDims = imageBlob->getTensorDesc().getDims();
                const auto imageWidth = imageDims[3];
                const auto imageHeight = imageDims[2];

                const auto& detectionsDims = detectionsBlob->getTensorDesc().getDims();
                const auto maxProposalCount = detectionsDims[2];
                const auto objectSize = detectionsDims[3];

                const auto detectionsMem = detectionsBlob->rmap();
                const auto detectionsPtr = detectionsMem.as<const float*>() +
                    detectionsBlob->getTensorDesc().getBlockingDesc().getOffsetPadding();

                for (size_t curProposal = 0; curProposal < maxProposalCount; curProposal++) {
                    const auto image_id = static_cast<int>(detectionsPtr[curProposal * objectSize + 0]);
                    if (image_id < 0) {
                        break;
                    }

                    const auto xmin = static_cast<size_t>(detectionsPtr[curProposal * objectSize + 3] * imageWidth);
                    const auto ymin = static_cast<size_t>(detectionsPtr[curProposal * objectSize + 4] * imageHeight);
                    const auto xmax = static_cast<size_t>(detectionsPtr[curProposal * objectSize + 5] * imageWidth);
                    const auto ymax = static_cast<size_t>(detectionsPtr[curProposal * objectSize + 6] * imageHeight);

                    ROI roi;
                    roi.id = image_id;
                    roi.posX = xmin;
                    roi.sizeX = xmax - xmin;
                    roi.posY = ymin;
                    roi.sizeY = ymax - ymin;

                    try {
                        ROIs.push_back(imageBlob->CreateROIBlob(roi));
                    } catch (const std::exception& ex) {
                        slog::err << "        Error creating ROI object at [" << roi.posX << ", " << roi.posY << " , "
                        << roi.posX + roi.sizeX << ", " << roi.posY + roi.sizeY << "] : " << ex.what() << slog::endl;
                    }
                }
                return ROIs;
            };

        const auto assignClassToROI =
            [&clsOutputName](
                    InferRequest& inferRequest,
                    size_t roiInd,
                    std::vector<int>& roiClasses) {
                const auto outputBlob = as<MemoryBlob>(inferRequest.GetBlob(clsOutputName) );

                const auto& dims = outputBlob->getTensorDesc().getDims();
                const auto numClasses = dims[1];

                const auto outputMem = outputBlob->rmap();
                const auto outputPtr = outputMem.as<const float*>() +
                    outputBlob->getTensorDesc().getBlockingDesc().getOffsetPadding();

                const auto topIter = std::max_element(outputPtr, outputPtr + numClasses);
                const auto topClassInd = std::distance(outputPtr, topIter);

                roiClasses[roiInd] = static_cast<int>(topClassInd);
            };

        const auto runClsInfers =
            [&clsInferRequests, &clsInputName, &assignClassToROI](
                    const std::vector<Blob::Ptr>& ROIs) -> std::vector<int> {
                std::vector<int> roiClasses(ROIs.size());
                const auto numRequests = clsInferRequests.size();

                std::vector<size_t> roiInds(numRequests);

                size_t infersStarted(0), infersFinished(0);
                std::mutex m;
                std::condition_variable cv;

                for (size_t inferInd = 0; inferInd < numRequests; ++inferInd) {
                    clsInferRequests[inferInd].SetCompletionCallback(
                        [&clsInferRequests, &clsInputName, &assignClassToROI,
                         &ROIs, &roiClasses,
                         &roiInds,
                         &m, &cv, &infersStarted, &infersFinished,
                         inferInd]() {
                            assignClassToROI(
                                clsInferRequests[inferInd],
                                roiInds[inferInd],
                                roiClasses);

                            std::unique_lock<std::mutex> lock(m);
                            ++infersFinished;
                            slog::info << "        Finish classification infer request #" << inferInd << " for ROI #" << roiInds[inferInd]
                                << " (of " << ROIs.size() << ")" << slog::endl;
                            if (infersStarted != ROIs.size()) {
                                slog::info << "        Start next classification infer request #" << inferInd << " for ROI #" << infersStarted
                                    << " (of " << ROIs.size() << ")" << slog::endl;

                                roiInds[inferInd] = infersStarted;
                                clsInferRequests[inferInd].SetBlob(clsInputName, ROIs[infersStarted]);
                                clsInferRequests[inferInd].StartAsync();
                                ++infersStarted;
                            } else {
                                lock.unlock();
                                cv.notify_one();
                            }
                        });
                }

                std::unique_lock<std::mutex> lock(m);
                infersStarted = std::min(numRequests, ROIs.size());

                for (size_t inferInd = 0; inferInd < infersStarted; ++inferInd) {
                    slog::info << "        Start classification infer request #" << inferInd << " for ROI #" << inferInd
                        << " (of " << ROIs.size() << ")" << slog::endl;

                    roiInds[inferInd] = inferInd;
                    clsInferRequests[inferInd].SetBlob(clsInputName, ROIs[inferInd]);
                    clsInferRequests[inferInd].StartAsync();
                }

                cv.wait(lock, [&infersFinished, &ROIs]{return infersFinished == ROIs.size();});

                return roiClasses;
            };

        const auto displayDetectionResults =
            [&clsLabels](
                    const std::vector<Blob::Ptr>& ROIs) {
                slog::info << "        Detection results (" << ROIs.size() << "):" << slog::endl;
                for (Blob::Ptr roiBlob : ROIs) {
                    const ROI& roi = roiBlob->getROI()->roi;
                    slog::info << "            Object at [" << roi.posX << ", " << roi.posY << " , "
                        << roi.posX + roi.sizeX << ", " << roi.posY + roi.sizeY << "];" << slog::endl;
                }
        };

        const auto displayClassificationResults =
            [&clsLabels](
                    const std::string& imageName,
                    MemoryBlob::Ptr imageBlob,
                    const std::vector<int>& boxes,
                    const std::vector<int>& classes) {
                slog::info << "        Classification results (" << classes.size() << "):" << slog::endl;
                for (size_t i = 0; i < classes.size(); i++) {
                    const auto x = boxes.at(i * 4 + 0);
                    const auto y = boxes.at(i * 4 + 1);
                    const auto w = boxes.at(i * 4 + 2);
                    const auto h = boxes.at(i * 4 + 3);

                    slog::info << "            Object at [" << x << ", " << y << " , " << x + w << ", " << y + h << "] : ";
                    if (clsLabels.empty()) {
                        slog::info << "class #" << classes[i];
                    } else {
                        slog::info << clsLabels.at(classes[i]);
                    }
                    slog::info << slog::endl;
                }

                const auto& curImageDims = imageBlob->getTensorDesc().getDims();
                const auto imageWidth = curImageDims[3];
                const auto imageHeight = curImageDims[2];

                const auto imageMem = imageBlob->rwmap();
                const auto imagePtr = imageMem.as<unsigned char*>() +
                    imageBlob->getTensorDesc().getBlockingDesc().getOffsetPadding();

                addRectangles(imagePtr, imageHeight, imageWidth, boxes, classes, BBOX_THICKNESS);

                const std::string outputFileName = getFileNameFromPath(imageName) + "_out.bmp";

                if (writeOutputBmp(outputFileName, imagePtr, imageHeight, imageWidth)) {
                    slog::info << "        Stored result to " << outputFileName << slog::endl;
                } else {
                    slog::err << "        Failed to write result to " << outputFileName << slog::endl;
                }
            };

        const auto getDetectionRectangles =
            [](const std::vector<Blob::Ptr>& ROIs)
                    -> std::vector<int> {
                std::vector<int> boxes;
                for (Blob::Ptr roiBlob : ROIs) {
                    const ROI& roi = roiBlob->getROI()->roi;
                    boxes.push_back(roi.posX);
                    boxes.push_back(roi.posY);
                    boxes.push_back(roi.sizeX);
                    boxes.push_back(roi.sizeY);
                }
                return boxes;
            };

        //
        // Run detection+classification pipeline
        //

        slog::info << "Run detection+classification pipeline" << slog::endl;

        const auto createImageBlob =
            [&imagesData, &imagesWidth, &imagesHeight](size_t imageInd) -> MemoryBlob::Ptr {
                const auto imageDataPtr = imagesData[imageInd].get();

                const auto imageWidth = imagesWidth[imageInd];
                const auto imageHeight = imagesHeight[imageInd];

                const auto blobDims = SizeVector {1, 3, imageHeight, imageWidth};
                const auto blobDesc = TensorDesc(Precision::U8, blobDims, Layout::NHWC);

                return make_shared_blob<uint8_t>(blobDesc, imageDataPtr);
            };

        for (size_t imageInd = 0; imageInd < images.size(); ++imageInd) {
            const auto& imageName = images[imageInd];
            const MemoryBlob::Ptr imageBlob = createImageBlob(imageInd);

            slog::info << "    Process " << imageName << slog::endl;

            detInferRequest.SetBlob(detInputName, imageBlob);
            detInferRequest.Infer();

            const std::vector<Blob::Ptr> ROIs = getDetectionROIs(as<MemoryBlob>(detInferRequest.GetBlob(detOutputName)), imageBlob);
            displayDetectionResults(ROIs);
            const std::vector<int> boxes = getDetectionRectangles(ROIs);
            const std::vector<int> classes = runClsInfers(ROIs);
            displayClassificationResults(imageName, imageBlob, boxes, classes);
        }
    }
    catch (const std::exception& error) {
        slog::err << error.what() << slog::endl;
        return 1;
    }
    catch (...) {
        slog::err << "Unknown/internal exception happened." << slog::endl;
        return 1;
    }

    slog::info << "Execution successful" << slog::endl;
    slog::info << slog::endl << "This sample is an API example, for any performance measurements "
                                "please use the dedicated benchmark_app tool" << slog::endl;

    return 0;
}